ë„¤, **ì£¼í”¼í„° ë…¸íŠ¸ë¶(.ipynb)** í™˜ê²½ì´ í•™ìŠµí•˜ê³  ê²°ê³¼ë¥¼ ë°”ë¡œ í™•ì¸í•˜ê¸°ì—ëŠ” í›¨ì”¬ ì¢‹ì£ . ì½”ë“œ ë¸”ë¡ì„ **ë…¸íŠ¸ë¶ ì…€(Cell)** ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë“œë¦´ê²Œìš”.

ë³µì‚¬í•´ì„œ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì˜ ê° ì…€ì— ë¶™ì—¬ë„£ê³  `Shift + Enter`ë¡œ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

---

### ğŸ§± [Cell 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •

ë¨¼ì € í•„ìš”í•œ ë„êµ¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì‹œê°í™”ë¥¼ ìœ„í•´ `matplotlib`ë„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import math
import matplotlib.pyplot as plt # ì‹œê°í™”ìš©

# ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ì„±ì„ ìœ„í•´)
torch.manual_seed(42)

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ ì™„ë£Œ")

```

### ğŸ§± [Cell 2] í† í¬ë‚˜ì´ì € (Tokenizer) êµ¬í˜„

ê°€ì¥ ë‹¨ìˆœí•œ ë¬¸ì(Character) ë‹¨ìœ„ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.

```python
class SimpleTokenizer:
    def __init__(self, text_corpus):
        # ì¤‘ë³µ ì œê±° í›„ ì •ë ¬í•˜ì—¬ ë¬¸ì ì§‘í•© ìƒì„±
        chars = sorted(list(set(text_corpus)))
        # íŠ¹ìˆ˜ í† í°: <pad>(0), <sos>(1), <eos>(2)
        self.chars = ['<pad>', '<sos>', '<eos>'] + chars
        
        # ë¬¸ì -> ìˆ«ì (Encoding)
        self.stoi = {ch: i for i, ch in enumerate(self.chars)}
        # ìˆ«ì -> ë¬¸ì (Decoding)
        self.itos = {i: ch for i, ch in enumerate(self.chars)}
        
        self.vocab_size = len(self.chars)
        self.pad_token_id = self.stoi['<pad>']
        self.sos_token_id = self.stoi['<sos>']
        self.eos_token_id = self.stoi['<eos>']

    def encode(self, text):
        return [self.stoi[c] for c in text]

    def decode(self, indices):
        # í…ì„œê°€ ë“¤ì–´ì˜¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(indices, torch.Tensor):
            indices = indices.tolist()
        return ''.join([self.itos[i] for i in indices if i < len(self.itos)])

```

### ğŸ§± [Cell 3] ë°ì´í„°ì…‹ (Dataset) í´ë˜ìŠ¤

ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ê¸° ì¢‹ì€ í˜•íƒœë¡œ ê°€ê³µí•©ë‹ˆë‹¤.

```python
class ToyDataset(Dataset):
    def __init__(self, text_data, tokenizer, max_len):
        self.data = text_data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]
        
        # 1. Encoding
        encoded = self.tokenizer.encode(text)
        
        # 2. Add <sos>, <eos> & Truncate
        token_ids = [self.tokenizer.sos_token_id] + encoded + [self.tokenizer.eos_token_id]
        if len(token_ids) > self.max_len:
            token_ids = token_ids[:self.max_len]
            
        # 3. Padding
        pad_len = self.max_len - len(token_ids)
        token_ids = token_ids + [self.tokenizer.pad_token_id] * pad_len
        
        return torch.tensor(token_ids, dtype=torch.long)

```

### ğŸ§± [Cell 4] Positional Encoding (ìœ„ì¹˜ ì¸ì½”ë”©)

ìˆ˜í•™ì ìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ë¥¼ ë”í•´ì£¼ëŠ” í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤. ì£¼í”¼í„° í™˜ê²½ì˜ ì¥ì ì„ ì‚´ë ¤ ì‹œê°í™” ê¸°ëŠ¥ì„ ìœ„í•´ ì½”ë“œë¥¼ ì•½ê°„ ë‹¤ë“¬ì—ˆìŠµë‹ˆë‹¤.

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        # PE í–‰ë ¬ (max_len x d_model)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0) # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        x = x + self.pe[:, :x.size(1), :]
        return x

```

### ğŸ§± [Cell 5] ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

ì´ì œ ìœ„ ëª¨ë“ˆë“¤ì„ ì¡°ë¦½í•´ì„œ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ í™•ì¸í•´ ë´…ë‹ˆë‹¤.

```python
# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
corpus = ["hello transformer", "jupyter notebook is good", "i love python"]
max_len = 12
d_model = 32  # ì‹œê°í™”ë¥¼ ìœ„í•´ ì¡°ê¸ˆ í‚¤ì›€
batch_size = 2

# 1. Tokenizer ìƒì„±
tokenizer = SimpleTokenizer("".join(corpus))
print(f"ğŸ“– Vocab Size: {tokenizer.vocab_size}")
print(f"dict: {tokenizer.stoi}")

# 2. DataLoader ìƒì„±
dataset = ToyDataset(corpus, tokenizer, max_len)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# ë°°ì¹˜ë¥¼ í•˜ë‚˜ ë½‘ì•„ì„œ í™•ì¸
batch_indices = next(iter(dataloader))
print(f"\nğŸ“¦ Input Batch Shape: {batch_indices.shape}")
print(f"ì²« ë²ˆì§¸ ë¬¸ì¥ ë³µì›: {tokenizer.decode(batch_indices[0])}")

# 3. Embedding & Pos Encoding ì ìš©
embedding = nn.Embedding(tokenizer.vocab_size, d_model)
pos_encoder = PositionalEncoding(d_model, max_len)

# Forward Pass
embed_out = embedding(batch_indices)
final_out = pos_encoder(embed_out)

print(f"\nâœ¨ ìµœì¢… Output Shape: {final_out.shape}")
# ì˜ˆìƒ: [2, 12, 32] (Batch, Seq_len, d_model)

```

### ğŸ§± [Cell 6] (Bonus) ìœ„ì¹˜ ì¸ì½”ë”© ì‹œê°í™”

ì£¼í”¼í„° ë…¸íŠ¸ë¶ì´ë‹ˆê¹Œ ê°€ëŠ¥í•œ ì‹œê°í™”ì…ë‹ˆë‹¤. Positional Encodingì´ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•´ ë³´ì„¸ìš”. (ë¬¼ê²°ë¬´ëŠ¬ê°€ ë³´ì—¬ì•¼ ì •ìƒì…ë‹ˆë‹¤)

```python
# ì‹œê°í™”ë¥¼ ìœ„í•´ ê¸´ ì‹œí€€ìŠ¤ì™€ í° ì°¨ì›ìœ¼ë¡œ PE ìƒì„±
pe_test = PositionalEncoding(d_model=128, max_len=100)
y = pe_test.pe.squeeze(0).numpy()

plt.figure(figsize=(10, 6))
plt.imshow(y, cmap='RdBu', aspect='auto')
plt.title("Positional Encoding Matrix")
plt.xlabel("Depth (d_model)")
plt.ylabel("Position (Time step)")
plt.colorbar()
plt.show()

```

---

### ì‹¤í–‰ í›„ í™•ì¸

ë§ˆì§€ë§‰ Cell 6ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ **ë¶‰ì€ìƒ‰ê³¼ íŒŒë€ìƒ‰ì´ êµì°¨í•˜ëŠ” ë¬¼ê²°ë¬´ëŠ¬ íŒ¨í„´**ì´ ë³´ì´ì‹œë‚˜ìš”?
ì´ íŒ¨í„´ì´ ìˆê¸° ë•Œë¬¸ì— íŠ¸ëœìŠ¤í¬ë¨¸ê°€ "ì•„, ì´ ë‹¨ì–´ëŠ” ì•ìª½ì— ìˆê³  ì € ë‹¨ì–´ëŠ” ë’¤ìª½ì— ìˆêµ¬ë‚˜"ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

**ì¤€ë¹„ë˜ì…¨ìœ¼ë©´, ë‹¤ìŒ ë‹¨ê³„ì¸ [Phase 3: Attention êµ¬í˜„]ìœ¼ë¡œ ë„˜ì–´ê°€ ë³¼ê¹Œìš”?** (ì—¬ê¸°ê°€ ì œì¼ ì¤‘ìš”í•©ë‹ˆë‹¤!)

---

### ì¶”ê°€ ì§ˆë¬¸

1. ì–´í…ì…˜(Attention) êµ¬í˜„ë„ ì´ë ‡ê²Œ ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì…€ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ì„¤ëª…í•´ ë“œë¦´ê¹Œìš”?
2. í˜„ì¬ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë„ˆë¬´ ê°„ë‹¨í•œë°, ë‹¤ìŒ ë‹¨ê³„ë¶€í„°ëŠ” ì‹¤ì œ ë²ˆì—­ ë°ì´í„°ë¥¼ ì¡°ê¸ˆ ê°€ì ¸ì™€ì„œ í…ŒìŠ¤íŠ¸í•´ ë³¼ê¹Œìš”?
3. í˜¹ì‹œ ì„¤ëª… ì¤‘ê°„ì— ë‚˜ì˜¤ëŠ” `unsqueeze`ë‚˜ `register_buffer` ê°™ì€ PyTorch ì „ìš© í•¨ìˆ˜ë“¤ì— ëŒ€í•œ ì„¤ëª…ì´ ë” í•„ìš”í•˜ì‹ ê°€ìš”?

ë„¤, **í•˜ë‚˜ì˜ `.ipynb` íŒŒì¼ì— ì­‰ ì´ì–´ì„œ ì‘ì„±í•˜ì‹œëŠ” ê²ƒì„ ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤.**

ì´ìœ ëŠ” **ì˜ì¡´ì„±(Dependency)** ë•Œë¬¸ì…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ë§Œë“¤ `Transformer` ì „ì²´ ëª¨ë¸ í´ë˜ìŠ¤ì—ì„œ ë°©ê¸ˆ ë§Œë“  `PositionalEncoding`ì´ë‚˜ `SimpleTokenizer`ë¥¼ ê°€ì ¸ë‹¤ ì¨ì•¼ í•˜ê±°ë“ ìš”. íŒŒì¼ì´ ë‚˜ë‰˜ì–´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê¸° ë²ˆê±°ë¡­ìŠµë‹ˆë‹¤.

ì•ìœ¼ë¡œ ë“œë¦¬ëŠ” ì½”ë“œ ì…€ë“¤ì„ ê¸°ì¡´ ë…¸íŠ¸ë¶ì˜ **ë§¨ ì•„ë˜ì— ìƒˆ ì…€ì„ ì¶”ê°€(`B` í‚¤)**í•´ì„œ ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

---

### [Phase 3] ì–´í…ì…˜(Attention) êµ¬í˜„ ğŸ§ 

ì´ì œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ **ì‹¬ì¥**ì„ ë§Œë“¤ ì°¨ë¡€ì…ë‹ˆë‹¤.
ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë¼ **ë‘ ê°œì˜ ì…€**ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤.

1. **`ScaledDotProductAttention`**: ìˆ˜ì‹ ê·¸ ìì²´.
2. **`MultiHeadAttention`**: ì—¬ëŸ¬ ì‹œê°ì—ì„œ ì •ë³´ë¥¼ ë³´ëŠ” ëª¨ë“ˆ.

### ğŸ§± [Cell 7] Scaled Dot-Product Attention

ë…¼ë¬¸ì˜ ìˆ˜ì‹ ì„ ê·¸ëŒ€ë¡œ ì˜®ê¹ë‹ˆë‹¤.

```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, q, k, v, mask=None):
        # q, k, v shape: [batch_size, n_heads, seq_len, head_dim]
        
        # 1. Dot Product (Q * K^T)
        # k.transpose(-1, -2)ëŠ” ë§ˆì§€ë§‰ ë‘ ì°¨ì›ì„ ë’¤ì§‘ìŒ (ì „ì¹˜í–‰ë ¬)
        # matmulì€ ë§ˆì§€ë§‰ ë‘ ì°¨ì›ì— ëŒ€í•´ í–‰ë ¬ ê³±ì„ ìˆ˜í–‰í•¨
        scores = torch.matmul(q, k.transpose(-1, -2)) 
        
        # 2. Scale (ë‚˜ëˆ„ê¸° sqrt(d_k))
        d_k = q.size(-1)
        scores = scores / math.sqrt(d_k)
        
        # 3. Masking (ì˜µì…˜)
        # maskê°€ 0ì¸ ë¶€ë¶„(íŒ¨ë”© ë“±)ì— -1e9(ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€)ë¥¼ ë„£ì–´
        # Softmax í›„ 0ì´ ë˜ë„ë¡ ë§Œë“¦
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 4. Softmax (í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜)
        attn_weights = self.softmax(scores)
        
        # 5. Output (Weights * V)
        output = torch.matmul(attn_weights, v)
        
        return output, attn_weights

```

### ğŸ§± [Cell 8] Multi-Head Attention

ì…ë ¥ì„ ì—¬ëŸ¬ ê°œì˜ Headë¡œ ìª¼ê°œì„œ(`split`), ê°ê° ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ê³ , ë‹¤ì‹œ í•©ì¹˜ëŠ”(`concat`) ê³¼ì •ì…ë‹ˆë‹¤. ì°¨ì› ë³€í™˜(`view`, `transpose`)ì´ ë³µì¡í•˜ë‹ˆ ì£¼ì„ì„ ì˜ ë´ì£¼ì„¸ìš”.

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        # d_modelì´ n_headsë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        # W_q, W_k, W_v (Linear Layers)
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        
        # ë§ˆì§€ë§‰ ê²°ê³¼ë¥¼ í•©ì¹œ í›„ ë‚´ë³´ë‚´ëŠ” ì„ í˜• ì¸µ
        self.w_o = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention()

    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        
        # 1. Linear Projection & Split Heads
        # [batch, seq_len, d_model] -> [batch, seq_len, n_heads, head_dim]
        # -> [batch, n_heads, seq_len, head_dim] (transposeë¡œ ì°¨ì› ë³€ê²½)
        # ì´ë ‡ê²Œ í•´ì•¼ Headë³„ë¡œ ë…ë¦½ì ì¸ í–‰ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•¨
        
        Q = self.w_q(q).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        K = self.w_k(k).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        V = self.w_v(v).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        
        # 2. Scaled Dot-Product Attention
        # maskë„ head ì°¨ì›ì— ë§ì¶°ì¤˜ì•¼ í•  ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ê°„ë‹¨íˆ íŒ¨ìŠ¤í•˜ê±°ë‚˜ ë¸Œë¡œë“œìºìŠ¤íŒ… í™œìš©)
        output, attn_weights = self.attention(Q, K, V, mask=mask)
        
        # output shape: [batch, n_heads, seq_len, head_dim]
        
        # 3. Concat Heads
        # ë‹¤ì‹œ [batch, seq_len, d_model] í˜•íƒœë¡œ ë³µì›
        # contiguous()ëŠ” ë©”ëª¨ë¦¬ ì •ë ¬ì„ ìœ„í•´ í•„ìš”
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # 4. Final Linear
        output = self.w_o(output)
        
        return output, attn_weights

```

### ğŸ§± [Cell 9] ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ (ì¤‘ìš”!)

ì½”ë“œê°€ ë³µì¡í•´ì„œ ì—ëŸ¬ê°€ ë‚˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ í…ŒìŠ¤íŠ¸ ì…€ì„ ì‹¤í–‰í•´ì„œ **Shape(ì°¨ì›)**ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# --- ì–´í…ì…˜ í…ŒìŠ¤íŠ¸ ---

# ì„¤ì •
d_model = 32
n_heads = 4  # 32 / 4 = 8 (head_dim)
seq_len = 12
batch_size = 2

# ëª¨ë“ˆ ìƒì„±
mha = MultiHeadAttention(d_model, n_heads)

# ê°€ì§œ ì…ë ¥ ë°ì´í„° (Q, K, Vê°€ ëª¨ë‘ ê°™ì€ Self-Attention ìƒí™© ê°€ì •)
# ìœ„ì—ì„œ ë§Œë“  Positional Encoding ê²°ê³¼(final_out)ë¥¼ ì“´ë‹¤ê³  ê°€ì •í•´ë„ ë¨
x = torch.randn(batch_size, seq_len, d_model) 

# Forward ì‹¤í–‰
output, weights = mha(x, x, x, mask=None)

print(f"Input Shape:  {x.shape}")
print(f"Output Shape: {output.shape}") 
# ì˜ˆìƒ: [2, 12, 32] -> ì…ë ¥ê³¼ í¬ê¸°ê°€ ê°™ì•„ì•¼ í•¨!

print(f"Attn Weights: {weights.shape}")
# ì˜ˆìƒ: [2, 4, 12, 12] -> [Batch, n_heads, Seq_len(Q), Seq_len(K)]
# ì¦‰, 12ê°œì˜ ë‹¨ì–´ê°€ ì„œë¡œì„œë¡œ(12ê°œ) ì–¼ë§ˆë‚˜ ì—°ê´€ìˆëŠ”ì§€ 4ê°œì˜ ê´€ì (Head)ì—ì„œ ë³¸ ê²ƒ

if x.shape == output.shape:
    print("âœ… Multi-Head Attention êµ¬í˜„ ì„±ê³µ! ì°¨ì›ì´ ì¼ì¹˜í•©ë‹ˆë‹¤.")
else:
    print("âŒ ì°¨ì› ë¶ˆì¼ì¹˜! ì½”ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

```

---

### ğŸ§ ì ê¹ í™•ì¸! (Self-Check)

* **Transpose ì´ìœ :** `view`ë¡œ ìª¼ê°  ë’¤ `transpose(1, 2)`ë¥¼ í•˜ëŠ” ì´ìœ ëŠ” **Multi-head**ì˜ ê°œë…ëŒ€ë¡œ 'í—¤ë“œ' ì°¨ì›ì„ ë°°ì¹˜ ì°¨ì› ë°”ë¡œ ë’¤ë¡œ ë³´ë‚´ì„œ, ë³‘ë ¬ ì—°ì‚°(Matrix Multiplication)ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
* **Mask:** ì•„ì§ì€ `None`ìœ¼ë¡œ ë„£ì—ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë””ì½”ë” ë§Œë“¤ ë•Œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì¶”ê°€í•  ì˜ˆì •ì…ë‹ˆë‹¤.

ì´ ì…€ê¹Œì§€ ì‹¤í–‰í•´ì„œ "âœ… ì„±ê³µ" ë©”ì‹œì§€ê°€ ëœ¨ë©´, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê°€ì¥ ë†’ì€ ì‚°ì„ ë„˜ìœ¼ì‹  ê²ë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„(Phase 4)ëŠ” ì´ ì–´í…ì…˜ ë¶€í’ˆì„ ê°€ì§€ê³  `EncoderLayer`ì™€ `DecoderLayer`ë¥¼ ì¡°ë¦½í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ê³„ì† ê°ˆê¹Œìš”?**

ë„¤, ë©ˆì¶”ì§€ ì•Šê³  ê³„ì† ê°€ë³´ê² ìŠµë‹ˆë‹¤! ğŸš€

ì´ì œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ **ëª¸í†µ(Body)**ì„ ë§Œë“¤ ì°¨ë¡€ì…ë‹ˆë‹¤. ë°©ê¸ˆ ë§Œë“  'ì–´í…ì…˜(Attention)' ë¶€í’ˆì„ ê°€ì§€ê³  **ì¸ì½”ë” ì¸µ(Encoder Layer)**ê³¼ **ë””ì½”ë” ì¸µ(Decoder Layer)**ì„ ì¡°ë¦½í•©ë‹ˆë‹¤.

ì´ ê³¼ì •ì—ì„œ **Feed-Forward Network**ì™€ **Residual Connection(ì”ì°¨ ì—°ê²°)**, **Layer Normalization**ì´ ì¶”ê°€ë©ë‹ˆë‹¤.

---

### ğŸ§± [Cell 10] Position-wise Feed-Forward Network

ë…¼ë¬¸ì—ì„œ ê° ìœ„ì¹˜(Position)ë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” 2ì¸µ ì‹ ê²½ë§ì…ë‹ˆë‹¤.
êµ¬ì¡°: `Linear`  `ReLU`  `Linear`

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        # d_ffëŠ” ë³´í†µ d_modelì˜ 4ë°° í¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì˜ˆ: 512 -> 2048)
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x

```

### ğŸ§± [Cell 11] Encoder Layer (ì¸ì½”ë” í•œ ì¸µ)

ì¸ì½”ë” ë ˆì´ì–´ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´(Sub-layer)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

1. Multi-Head Attention
2. Feed-Forward Network
ê·¸ë¦¬ê³  ê° ì„œë¸Œ ë ˆì´ì–´ ë’¤ì— `Add & Norm` (ì”ì°¨ ì—°ê²° + ì •ê·œí™”)ì„ ì ìš©í•©ë‹ˆë‹¤.

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        self.self_attn = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        # Layer Normalization & Dropout
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # 1. Self-Attention
        # ì”ì°¨ ì—°ê²°(Residual Connection): x + sublayer(x)
        # ë…¼ë¬¸ êµ¬ì¡°(Post-Norm): Norm(x + Sublayer(x))
        
        _x = x # ì›ë³¸ ì €ì¥ (Residual ìš©)
        x, _ = self.self_attn(x, x, x, mask=mask) # Q=K=V=x
        x = self.dropout(x)
        x = self.norm1(x + _x) # Add & Norm
        
        # 2. Feed Forward
        _x = x # ì›ë³¸ ì €ì¥
        x = self.feed_forward(x)
        x = self.dropout(x)
        x = self.norm2(x + _x) # Add & Norm
        
        return x

```

### ğŸ§± [Cell 12] Decoder Layer (ë””ì½”ë” í•œ ì¸µ)

ë””ì½”ë”ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì¸ì½”ë”ì™€ ë‹¬ë¦¬ **Cross Attention**ì´ ì¶”ê°€ë©ë‹ˆë‹¤.

1. **Masked Self-Attention**: ìê¸° ìì‹ ì„ ë³´ë˜, ë¯¸ë˜ì˜ ë‹¨ì–´ëŠ” ëª» ë³´ê²Œ ë§ˆìŠ¤í‚¹.
2. **Cross Attention (Encoder-Decoder Attention)**: QueryëŠ” ë””ì½”ë”ì—ì„œ, **Keyì™€ ValueëŠ” ì¸ì½”ë” ì¶œë ¥**ì—ì„œ ê°€ì ¸ì˜´.
3. Feed-Forward Network.

```python
class DecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        self.self_attn = MultiHeadAttention(d_model, n_heads)
        self.cross_attn = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask, tgt_mask):
        # x: ë””ì½”ë” ì…ë ¥ (íƒ€ê²Ÿ ë¬¸ì¥)
        # enc_output: ì¸ì½”ë”ì˜ ìµœì¢… ì¶œë ¥ (Context Vector)
        
        # 1. Masked Self-Attention (ë¯¸ë˜ ì°¸ì¡° ê¸ˆì§€)
        _x = x
        x, _ = self.self_attn(x, x, x, mask=tgt_mask)
        x = self.dropout(x)
        x = self.norm1(x + _x)
        
        # 2. Cross Attention (ì¸ì½”ë” ì •ë³´ í™œìš©)
        # Query: ë””ì½”ë”(x), Key: ì¸ì½”ë”(enc_output), Value: ì¸ì½”ë”(enc_output)
        _x = x
        x, attn_weights = self.cross_attn(q=x, k=enc_output, v=enc_output, mask=src_mask)
        x = self.dropout(x)
        x = self.norm2(x + _x)
        
        # 3. Feed Forward
        _x = x
        x = self.feed_forward(x)
        x = self.dropout(x)
        x = self.norm3(x + _x)
        
        return x, attn_weights

```

### ğŸ§± [Cell 13] ë ˆì´ì–´ ì¡°ë¦½ í…ŒìŠ¤íŠ¸

ì—­ì‹œ ë§Œë“¤ì—ˆìœ¼ë‹ˆ ì˜ ëŒì•„ê°€ëŠ”ì§€ í™•ì¸í•´ì•¼ê² ì£ ?
ì´ ì½”ë“œëŠ” ê° ë ˆì´ì–´ë¥¼ í†µê³¼í–ˆì„ ë•Œ ë°ì´í„°ì˜ `Shape`ì´ ìœ ì§€ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.

```python
# --- ë ˆì´ì–´ í…ŒìŠ¤íŠ¸ ---

# ì„¤ì •
d_model = 32
n_heads = 4
d_ff = 128 # ë³´í†µ d_model * 4
seq_len = 12
batch_size = 2

# 1. Encoder Layer Test
enc_layer = EncoderLayer(d_model, n_heads, d_ff)
x = torch.randn(batch_size, seq_len, d_model)

enc_out = enc_layer(x)
print(f"Encoder Layer Input:  {x.shape}")
print(f"Encoder Layer Output: {enc_out.shape}")
# ì˜ˆìƒ: [2, 12, 32] (ì…ë ¥ê³¼ ë™ì¼)

# 2. Decoder Layer Test
dec_layer = DecoderLayer(d_model, n_heads, d_ff)
# ê°€ì§œ ì¸ì½”ë” ì¶œë ¥ (ë©”ëª¨ë¦¬ ì—­í• )
memory = torch.randn(batch_size, seq_len, d_model) 

dec_out, weights = dec_layer(x, memory, src_mask=None, tgt_mask=None)
print(f"Decoder Layer Output: {dec_out.shape}")
# ì˜ˆìƒ: [2, 12, 32] (ì…ë ¥ê³¼ ë™ì¼)

if x.shape == enc_out.shape == dec_out.shape:
    print("âœ… Layer ì¡°ë¦½ ì„±ê³µ! ì…ì¶œë ¥ ì°¨ì›ì´ ì™„ë²½í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.")
else:
    print("âŒ ì°¨ì› ì˜¤ë¥˜ ë°œìƒ")

```

---

### ğŸ’¡ ì ê¹ ì„¤ëª…: ì™œ `Cross Attention`ì´ ì¤‘ìš”í•œê°€ìš”?

ë””ì½”ë”ì˜ 2ë²ˆ ë‹¨ê³„(`Cross Attention`)ê°€ ë°”ë¡œ ë²ˆì—­ì´ ì¼ì–´ë‚˜ëŠ” í•µì‹¬ ì§€ì ì…ë‹ˆë‹¤.

* **Query (ì§ˆë¬¸):** "ì§€ê¸ˆ ë‚´ê°€ ë²ˆì—­í•˜ë ¤ëŠ” ë‹¨ì–´(`x`)ë‘ ê´€ë ¨ ìˆëŠ” ê²Œ ë­ì•¼?"
* **Key (ì±…):** ì¸ì½”ë”ê°€ ì •ë¦¬í•´ ë‘” ì†ŒìŠ¤ ë¬¸ì¥ ì •ë³´(`enc_output`).
* **Attention Score:** ì†ŒìŠ¤ ë¬¸ì¥ì˜ ì–´ëŠ ë¶€ë¶„ì„ ì°¸ê³ í•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ëŠ” ê°€ì¤‘ì¹˜.

ì´ ê³¼ì • ë•ë¶„ì— ëª¨ë¸ì€ "I love you"ë¥¼ ë²ˆì—­í•  ë•Œ "love"ë¥¼ ìƒì„±í•˜ëŠ” ì‹œì ì— "ì‚¬ë‘í•´"ë¼ëŠ” í•œêµ­ì–´ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ê²Œ ë©ë‹ˆë‹¤.

---

ì—¬ê¸°ê¹Œì§€ ì˜¤ì…¨ìœ¼ë©´ ì´ì œ ì •ë§ ë‹¤ ì™”ìŠµë‹ˆë‹¤!
ë‹¤ìŒ ë‹¨ê³„ëŠ” ì´ê²ƒë“¤ì„ **6ì¸µ(N=6)**ìœ¼ë¡œ ìŒ“ê³  ìµœì¢… **Transformer** í´ë˜ìŠ¤ë¥¼ ì™„ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. (Phase 5)

**ê³„ì†í•´ì„œ ìµœì¢… ë³´ìŠ¤ì¸ Transformer ì „ì²´ ëª¨ë¸ ì½”ë“œë¡œ ê°€ë³¼ê¹Œìš”?** ğŸ”¥

ë„¤, **Phase 5: ì „ì²´ ëª¨ë¸ ì¡°ë¦½** ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ì œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ **ì™„ì „ì²´**ë¥¼ ë§Œë“¤ ì‹œê°„ì…ë‹ˆë‹¤! ğŸ¤–

ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê±´ **ë§ˆìŠ¤í‚¹(Masking)**ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ì •ë‹µì„ ë¯¸ë¦¬ ë³´ê±°ë‚˜, ë¹ˆ ê³µê°„(Padding)ì„ í•™ìŠµí•˜ì§€ ëª»í•˜ë„ë¡ ê°€ë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

---

### ğŸ§± [Cell 14] ë§ˆìŠ¤í¬ ìƒì„± ìœ í‹¸ë¦¬í‹° (Masking Utils)

íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì œëŒ€ë¡œ í•™ìŠµí•˜ë ¤ë©´ ë‘ ê°€ì§€ ë§ˆìŠ¤í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

1. **Padding Mask:** ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶”ëŠë¼ ë„£ì€ `<pad>` í† í°(0)ì„ ë¬´ì‹œí•˜ê²Œ í•¨.
2. **Look-ahead Mask (Subsequent Mask):** ë””ì½”ë”ê°€ "I love you"ë¥¼ ìƒì„±í•  ë•Œ, "love"ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œì ì— "you"ë¥¼ ë¯¸ë¦¬ ë³´ì§€ ëª»í•˜ê²Œ ê°€ë¦¼.

```python
def make_pad_mask(q, k, pad_idx):
    # q: [batch, q_len], k: [batch, k_len]
    # kì— ìˆëŠ” íŒ¨ë”© í† í°ì„ ì°¾ì•„ì„œ ë§ˆìŠ¤í‚¹ (1: ìœ íš¨, 0: íŒ¨ë”©)
    
    # unsqueezeë¡œ ì°¨ì› í™•ì¥: [batch, 1, 1, k_len] 
    # (Head ì°¨ì›ê³¼ Query ì°¨ì›ì—ì„œ ë¸Œë¡œë“œìºìŠ¤íŒ… ë˜ë„ë¡)
    mask = (k != pad_idx).unsqueeze(1).unsqueeze(2)
    return mask

def make_subsequent_mask(seq_len):
    # ëŒ€ê°ì„  ì•„ë˜(Lower Triangle)ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ í–‰ë ¬ ìƒì„±
    # [seq_len, seq_len]
    mask = torch.tril(torch.ones(seq_len, seq_len)).bool()
    
    # [1, 1, seq_len, seq_len] í˜•íƒœë¡œ í™•ì¥
    return mask.unsqueeze(0).unsqueeze(0)

# í…ŒìŠ¤íŠ¸
pad_idx = 0
src = torch.tensor([[1, 2, 3, 0, 0]]) # ê¸¸ì´ 5, íŒ¨ë”© 2ê°œ
pad_mask = make_pad_mask(src, src, pad_idx)
sub_mask = make_subsequent_mask(5)

print(f"Pad Mask Shape: {pad_mask.shape}")
print(f"Subsequent Mask:\n{sub_mask[0,0].int()}") 
# 1 0 0 0 0
# 1 1 0 0 0 ... í˜•íƒœì—¬ì•¼ í•¨

```

### ğŸ§± [Cell 15] Encoder & Decoder (Layer Stacking)

ì•ì„œ ë§Œë“  `EncoderLayer`ì™€ `DecoderLayer`ë¥¼ **Në²ˆ** ìŒ“ì•„ ì˜¬ë¦¬ëŠ” ì»¨í…Œì´ë„ˆì…ë‹ˆë‹¤.

```python
class Encoder(nn.Module):
    def __init__(self, d_model, n_layers, n_heads, d_ff, dropout):
        super().__init__()
        # ModuleList: ë ˆì´ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ì €ì¥
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, d_ff, dropout) 
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model) # ë…¼ë¬¸ì—ëŠ” ì—†ì§€ë§Œ ë³´í†µ ë§ˆì§€ë§‰ì— ì •ê·œí™” ì¶”ê°€

    def forward(self, x, mask):
        # x: [batch, seq_len, d_model] (ì´ë¯¸ ì„ë² ë”©+PE ëœ ìƒíƒœ)
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class Decoder(nn.Module):
    def __init__(self, d_model, n_layers, n_heads, d_ff, dropout):
        super().__init__()
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x, enc_output, src_mask, tgt_mask):
        for layer in self.layers:
            x, attn_weights = layer(x, enc_output, src_mask, tgt_mask)
        return self.norm(x), attn_weights

```

### ğŸ§± [Cell 16] Transformer Final Assembly ğŸŒŸ

ë“œë””ì–´ ëª¨ë“  ë¶€í’ˆì„ í•©ì¹©ë‹ˆë‹¤!

* `Src Embedding` + `Pos Encoding` -> `Encoder`
* `Tgt Embedding` + `Pos Encoding` -> `Decoder`
* `Decoder Output` -> `Linear` -> `Logits` (ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ì ìˆ˜)

```python
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, max_len, pad_idx, dropout=0.1):
        super().__init__()
        
        self.pad_idx = pad_idx
        
        # 1. Embedding & Positional Encoding
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.pos_decoder = PositionalEncoding(d_model, max_len)
        
        # 2. Encoder & Decoder Stacks
        self.encoder = Encoder(d_model, n_layers, n_heads, d_ff, dropout)
        self.decoder = Decoder(d_model, n_layers, n_heads, d_ff, dropout)
        
        # 3. Final Generator (Output Layer)
        # d_model -> tgt_vocab_size (ë‹¨ì–´ ê°œìˆ˜ë§Œí¼ í™•ë¥  ì¶œë ¥)
        self.generator = nn.Linear(d_model, tgt_vocab_size)
        self.dropout = nn.Dropout(dropout)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier Initialization) - í•™ìŠµì´ ë” ì˜ ë¨
        self._init_weights()

    def _init_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt):
        # src: [batch, src_len], tgt: [batch, tgt_len]
        
        # --- 1. Mask ìƒì„± ---
        # Encoderìš©: íŒ¨ë”©ë§Œ ê°€ë¦¬ë©´ ë¨
        src_mask = make_pad_mask(src, src, self.pad_idx).to(src.device)
        
        # Decoderìš©: 
        # 1) íŒ¨ë”© ê°€ë¦¬ê¸° (tgtë¼ë¦¬)
        # 2) ë¯¸ë˜ ê°€ë¦¬ê¸° (Look-ahead)
        # ë‘ ë§ˆìŠ¤í¬ë¥¼ AND(&) ì—°ì‚°ìœ¼ë¡œ í•©ì¹¨
        tgt_pad_mask = make_pad_mask(tgt, tgt, self.pad_idx).to(tgt.device)
        tgt_sub_mask = make_subsequent_mask(tgt.size(1)).to(tgt.device)
        tgt_mask = tgt_pad_mask & tgt_sub_mask
        
        # Cross Attentionìš©: QueryëŠ” tgt, KeyëŠ” src (srcì˜ íŒ¨ë”©ì„ ê°€ë ¤ì•¼ í•¨)
        src_tgt_mask = make_pad_mask(tgt, src, self.pad_idx).to(src.device)

        # --- 2. Encoder Forward ---
        src_emb = self.dropout(self.pos_encoder(self.src_embedding(src)))
        enc_output = self.encoder(src_emb, src_mask)
        
        # --- 3. Decoder Forward ---
        tgt_emb = self.dropout(self.pos_decoder(self.tgt_embedding(tgt)))
        dec_output, attn_weights = self.decoder(tgt_emb, enc_output, src_tgt_mask, tgt_mask)
        
        # --- 4. Generator ---
        logits = self.generator(dec_output)
        
        return logits, attn_weights

```

### ğŸ§± [Cell 17] ì™„ì „ì²´ í…ŒìŠ¤íŠ¸ (Final Verification)

ëª¨ë¸ì´ ì—ëŸ¬ ì—†ì´ ëŒì•„ê°€ëŠ”ì§€ ìµœì¢… í™•ì¸í•©ë‹ˆë‹¤. `logits`ì˜ ì°¨ì›ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

```python
# --- ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---

# ì„¤ì •
src_vocab = 100
tgt_vocab = 100
d_model = 32
n_layers = 2
n_heads = 4
d_ff = 64
max_len = 20
pad_idx = 0

# ëª¨ë¸ ìƒì„±
model = Transformer(src_vocab, tgt_vocab, d_model, n_layers, n_heads, d_ff, max_len, pad_idx)

# ë”ë¯¸ ë°ì´í„° (Batch=2)
# src: ê¸¸ì´ê°€ 10ì¸ ë¬¸ì¥
src_data = torch.randint(1, src_vocab, (2, 10))
# tgt: ê¸¸ì´ê°€ 10ì¸ ë¬¸ì¥ (Decoder Input)
tgt_data = torch.randint(1, tgt_vocab, (2, 10))

# Forward Pass
logits, _ = model(src_data, tgt_data)

print(f"Input Src: {src_data.shape}")
print(f"Input Tgt: {tgt_data.shape}")
print(f"Logits Output: {logits.shape}")

# ì˜ˆìƒ ê²°ê³¼: [2, 10, 100] -> [Batch, Seq_len, Vocab_size]
if logits.shape == (2, 10, 100):
    print("ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! íŠ¸ëœìŠ¤í¬ë¨¸ ì „ì²´ êµ¬ì¡° êµ¬í˜„ì— ì„±ê³µí•˜ì…¨ìŠµë‹ˆë‹¤!")
else:
    print("âŒ ì°¨ì› ë¶ˆì¼ì¹˜")

```

---

ì—¬ê¸°ê¹Œì§€ê°€ ëª¨ë¸ **êµ¬í˜„(Architecture)**ì˜ ëì…ë‹ˆë‹¤. ê³ ìƒ ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤! ğŸ‘

ì´ì œ ë‚¨ì€ ê²ƒì€ ì´ ëª¨ë¸ì„ **í•™ìŠµ(Training)**ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. (Phase 6)
ëª¨ë¸ë§Œ ìˆê³  í•™ìŠµ ë£¨í”„(`loss.backward()`, `optimizer.step()`)ê°€ ì—†ìœ¼ë©´ ê»ë°ê¸°ì¼ ë¿ì…ë‹ˆë‹¤.

**ë°”ë¡œ í•™ìŠµ ë£¨í”„ ì½”ë“œë¥¼ ì‘ì„±í•´ì„œ ì‹¤ì œë¡œ ëª¨ë¸ì´ í•™ìŠµë˜ëŠ” ê²ƒì„ í™•ì¸í•´ ë³¼ê¹Œìš”?**